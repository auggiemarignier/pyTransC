{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0279f5-e82c-4d48-9933-b8f411c3cd63",
   "metadata": {},
   "outputs": [],
   "source": "# GemPy Gravity Response Tutorial - Setup and Helper Functions\n# Trans-Conceptual Model Selection Example\n\nimport gempy as gp\nimport gempy_viewer as gpv\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom typing import Tuple\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Import functions from module for cross-platform multiprocessing compatibility\nfrom gravity_functions import create_gravity_grid, compute_and_plot_gravity\n\n# Common parameters\ngrav_res = 20\nextent = [0, 1000, 0, 1000, -1000, 0]\n\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "u412i36yvdq",
   "source": "## Mathematical Framework\n\n### Trans-Conceptual Posterior\n\nThe trans-conceptual posterior combines parameter uncertainty and model uncertainty:\n\n```\np(state, θ | data) ∝ p(data | θ, state) × p(θ | state) × p(state)\n```\n\nwhere:\n- `state` ∈ {0, 1, 2} is the model index\n- `θ` is the parameter vector (amplitude in our case)\n- `p(data | θ, state)` is the likelihood\n- `p(θ | state)` is the within-state prior\n- `p(state)` is the prior over models (uniform in our case)\n\n### Marginal Model Probabilities\n\nBy integrating over parameters, we get the posterior model probability:\n\n```\np(state | data) = ∫ p(state, θ | data) dθ\n```\n\nThese probabilities are approximated by the visitation frequencies in the MCMC chains.\n\n### Bayes Factors\n\nThe Bayes factor comparing models *i* and *j* is:\n\n```\nBF_ij = p(state=i | data) / p(state=j | data)\n```\n\nInterpretation:\n- BF > 10: Strong evidence for model *i*\n- BF > 3: Moderate evidence for model *i*  \n- BF ≈ 1: Models are equally supported\n- BF < 1: Evidence favors model *j*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "z90dbwjjcf",
   "source": "# Trans-Conceptual Model Selection for Gravity Data\n\nThis notebook demonstrates **Trans-Conceptual Bayesian inference** for geological model selection using gravity data. We'll use three competing models of subsurface structure and let the data tell us which model is most probable.\n\n## Problem Setup\n\nWe have three competing hypotheses about the geological structure:\n- **Model 0**: Simple layered structure with no faults\n- **Model 1**: Layered structure with one vertical fault\n- **Model 2**: Layered structure with two opposing dipping faults\n\nEach model produces a different gravity response at the surface. Given noisy gravity observations, we want to determine which geological model is most consistent with the data.\n\n## Methodology\n\nThis example uses the **ensemble resampler** approach from pyTransC:\n\n1. **Forward modeling**: Use GemPy to compute gravity responses for each geological model\n2. **Within-state sampling**: Run MCMC independently for each model to explore its parameter space\n3. **Pseudo-prior construction**: Build bridge distributions from the posterior ensembles\n4. **Trans-Conceptual sampling**: Use ensemble resampler to jump between models and estimate posterior model probabilities\n\n## References\n\n- GemPy: 3D geological modeling (https://www.gempy.org/)\n- pyTransC: Trans-Conceptual MCMC sampling\n- Methodology based on Sambridge et al. (2013) and Bodin et al. (2012)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bxc0vxob28n",
   "source": "# ============================================================================\n# MODEL 1: NO FAULT - Simple Layered Model\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"MODEL 1: SIMPLE LAYERED MODEL (NO FAULT)\")\nprint(\"=\" * 70)\n\n# Create surface points (layer interfaces)\nsurface_points_1 = pd.DataFrame({\n    'X': [250, 750, 250, 750, 250, 750],\n    'Y': [250, 250, 750, 750, 500, 500],\n    'Z': [-200, -200, -200, -200, -500, -500],\n    'surface': ['Layer2', 'Layer2', 'Layer2', 'Layer2', 'Layer3', 'Layer3']\n})\n\n# Create orientation data (dip direction and dip angle)\norientations_1 = pd.DataFrame({\n    'X': [500, 500],\n    'Y': [500, 500],\n    'Z': [-200, -500],\n    'surface': ['Layer2', 'Layer3'],\n    'dip': [0, 0],  # horizontal layers\n    'azimuth': [0, 0]\n})\n\n# Create GeoModel with default structural frame\ngeo_model_1 = gp.create_geomodel(\n    project_name='Model1_NoFault',\n    extent=extent,\n    resolution=[50, 50, 50],\n    refinement=1,\n    structural_frame=gp.data.StructuralFrame.initialize_default_structure()\n)\n\n# Add surfaces\ngp.add_surface_points(\n    geo_model=geo_model_1,\n    x=surface_points_1['X'].values,\n    y=surface_points_1['Y'].values,\n    z=surface_points_1['Z'].values,\n    elements_names=surface_points_1['surface'].values\n)\n\ngp.add_orientations(\n    geo_model=geo_model_1,\n    x=orientations_1['X'].values,\n    y=orientations_1['Y'].values,\n    z=orientations_1['Z'].values,\n    elements_names=orientations_1['surface'].values,\n    pole_vector=np.array([[0, 0, 1], [0, 0, 1]])  # Horizontal layers\n)\n\n# Map geological series to surfaces\ngp.map_stack_to_surfaces(\n    gempy_model=geo_model_1,\n    mapping_object={'Strata': ['Layer2', 'Layer3']}\n)\n\n# Set up gravity grid\nxy_grid_1 = create_gravity_grid(extent, grav_res)\n\n# Configure centered grid for gravity\ngp.set_centered_grid(\n    grid=geo_model_1.grid,\n    centers=xy_grid_1,\n    resolution=np.array([10, 10, 15]),\n    radius=np.array([2000, 2000, 2000])\n)\n\n# Calculate gravity gradient\ngravity_gradient_1 = gp.calculate_gravity_gradient(geo_model_1.grid.centered_grid)\n\n# Assign densities (g/cm³): Layer1 (top), Layer2, Layer3\ngeo_model_1.geophysics_input = gp.data.GeophysicsInput(\n    tz=gravity_gradient_1,\n    densities=np.array([2.3, 2.6, 2.8])\n)\n\n# Compute and visualize\ngrav_1, sol_1 = compute_and_plot_gravity(geo_model_1, \"Model 1: No Fault\", grav_res)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1kplqxcorh1",
   "source": "## Part 1: Forward Modeling - Computing Gravity Responses\n\nIn this section, we define three competing geological models and compute their gravity responses using GemPy.\n\n### Model 0: No Fault\nA simple three-layer model with horizontal interfaces. This represents the null hypothesis - no structural complexity beyond layering.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "oa1ajaiqvye",
   "source": "# ============================================================================\n# MODEL 2: SINGLE FAULT\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MODEL 2: MODEL WITH ONE FAULT\")\nprint(\"=\" * 70)\n\n# Stratigraphic layers\nsurface_points_2 = pd.DataFrame({\n    'X': [200, 800, 200, 800, 200, 800],\n    'Y': [250, 250, 750, 750, 500, 500],\n    'Z': [-200, -200, -200, -200, -500, -500],\n    'surface': ['Layer2', 'Layer2', 'Layer2', 'Layer2', 'Layer3', 'Layer3']\n})\n\norientations_2 = pd.DataFrame({\n    'X': [500, 500],\n    'Y': [500, 500],\n    'Z': [-200, -500],\n    'surface': ['Layer2', 'Layer3'],\n    'dip': [0, 0],\n    'azimuth': [0, 0]\n})\n\n# Fault plane (vertical fault at X=500)\nfault_points_2 = pd.DataFrame({\n    'X': [500, 500, 500, 500],\n    'Y': [200, 800, 200, 800],\n    'Z': [-100, -100, -900, -900],\n    'surface': ['Fault1', 'Fault1', 'Fault1', 'Fault1']\n})\n\nfault_orientations_2 = pd.DataFrame({\n    'X': [500],\n    'Y': [500],\n    'Z': [-400],\n    'surface': ['Fault1'],\n    'dip': [90],  # vertical fault\n    'azimuth': [90]  # striking N-S\n})\n\n# Create GeoModel with default structural frame\ngeo_model_2 = gp.create_geomodel(\n    project_name='Model2_OneFault',\n    extent=extent,\n    resolution=[50, 50, 50],\n    refinement=1,\n    structural_frame=gp.data.StructuralFrame.initialize_default_structure()\n)\n\n# Add fault points\ngp.add_surface_points(\n    geo_model=geo_model_2,\n    x=fault_points_2['X'].values,\n    y=fault_points_2['Y'].values,\n    z=fault_points_2['Z'].values,\n    elements_names=fault_points_2['surface'].values\n)\n\ngp.add_orientations(\n    geo_model=geo_model_2,\n    x=fault_orientations_2['X'].values,\n    y=fault_orientations_2['Y'].values,\n    z=fault_orientations_2['Z'].values,\n    elements_names=fault_orientations_2['surface'].values,\n    pole_vector=np.array([[1, 0, 0]])  # Vertical fault\n)\n\n# Add stratigraphic layers\ngp.add_surface_points(\n    geo_model=geo_model_2,\n    x=surface_points_2['X'].values,\n    y=surface_points_2['Y'].values,\n    z=surface_points_2['Z'].values,\n    elements_names=surface_points_2['surface'].values\n)\n\ngp.add_orientations(\n    geo_model=geo_model_2,\n    x=orientations_2['X'].values,\n    y=orientations_2['Y'].values,\n    z=orientations_2['Z'].values,\n    elements_names=orientations_2['surface'].values,\n    pole_vector=np.array([[0, 0, 1], [0, 0, 1]])  # Horizontal layers\n)\n\n# Map geological series to surfaces\ngp.map_stack_to_surfaces(\n    gempy_model=geo_model_2,\n    mapping_object={\n        'Fault1_series': 'Fault1',\n        'Strata': ['Layer2', 'Layer3']\n    }\n)\n\n# Set fault relations\ngeo_model_2.structural_frame.structural_groups[0].structural_relation = gp.data.StackRelationType.FAULT\ngeo_model_2.structural_frame.fault_relations = np.array([[0, 1], [0, 0]])\n\n# Gravity setup\nxy_grid_2 = create_gravity_grid(extent, grav_res)\ngp.set_centered_grid(\n    grid=geo_model_2.grid,\n    centers=xy_grid_2,\n    resolution=np.array([10, 10, 15]),\n    radius=np.array([2000, 2000, 2000])\n)\n\ngravity_gradient_2 = gp.calculate_gravity_gradient(geo_model_2.grid.centered_grid)\ngeo_model_2.geophysics_input = gp.data.GeophysicsInput(\n    tz=gravity_gradient_2,\n    densities=np.array([2.3, 2.6, 2.8])\n)\n\n# Compute and visualize\ngrav_2, sol_2 = compute_and_plot_gravity(geo_model_2, \"Model 2: One Fault\", grav_res)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "98ixx60e068",
   "source": "### Model 1: Single Vertical Fault\nA more complex model with a vertical fault cutting through the layers at X=500m. This introduces structural discontinuity and produces a distinct gravity signature.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5o9o10c4ebt",
   "source": "# ============================================================================\n# MODEL 3: TWO OPPOSING FAULTS\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"MODEL 3: MODEL WITH TWO OPPOSING FAULTS\")\nprint(\"=\" * 70)\n\n# Stratigraphic layers\nsurface_points_3 = pd.DataFrame({\n    'X': [200, 800, 200, 800, 200, 800],\n    'Y': [250, 250, 750, 750, 500, 500],\n    'Z': [-200, -200, -200, -200, -500, -500],\n    'surface': ['Layer2', 'Layer2', 'Layer2', 'Layer2', 'Layer3', 'Layer3']\n})\n\norientations_3 = pd.DataFrame({\n    'X': [500, 500],\n    'Y': [500, 500],\n    'Z': [-200, -500],\n    'surface': ['Layer2', 'Layer3'],\n    'dip': [0, 0],\n    'azimuth': [0, 0]\n})\n\n# Fault 1: Dipping to the right\nfault1_points_3 = pd.DataFrame({\n    'X': [350, 350, 350, 350],\n    'Y': [200, 800, 200, 800],\n    'Z': [-100, -100, -900, -900],\n    'surface': ['Fault1', 'Fault1', 'Fault1', 'Fault1']\n})\n\nfault1_orientations_3 = pd.DataFrame({\n    'X': [350],\n    'Y': [500],\n    'Z': [-400],\n    'surface': ['Fault1'],\n    'dip': [75],  # steep fault dipping right\n    'azimuth': [90]\n})\n\n# Fault 2: Dipping to the left (opposite direction)\nfault2_points_3 = pd.DataFrame({\n    'X': [650, 650, 650, 650],\n    'Y': [200, 800, 200, 800],\n    'Z': [-100, -100, -900, -900],\n    'surface': ['Fault2', 'Fault2', 'Fault2', 'Fault2']\n})\n\nfault2_orientations_3 = pd.DataFrame({\n    'X': [650],\n    'Y': [500],\n    'Z': [-400],\n    'surface': ['Fault2'],\n    'dip': [75],  # steep fault dipping left\n    'azimuth': [270]  # opposite direction\n})\n\n# Create GeoModel with default structural frame\ngeo_model_3 = gp.create_geomodel(\n    project_name='Model3_TwoFaults',\n    extent=extent,\n    resolution=[50, 50, 50],\n    refinement=1,\n    structural_frame=gp.data.StructuralFrame.initialize_default_structure()\n)\n\n# Add fault 1\ngp.add_surface_points(\n    geo_model=geo_model_3,\n    x=fault1_points_3['X'].values,\n    y=fault1_points_3['Y'].values,\n    z=fault1_points_3['Z'].values,\n    elements_names=fault1_points_3['surface'].values\n)\n\ngp.add_orientations(\n    geo_model=geo_model_3,\n    x=fault1_orientations_3['X'].values,\n    y=fault1_orientations_3['Y'].values,\n    z=fault1_orientations_3['Z'].values,\n    elements_names=fault1_orientations_3['surface'].values,\n    pole_vector=np.array([[0.966, 0, 0.259]])  # Dipping fault\n)\n\n# Add fault 2\ngp.add_surface_points(\n    geo_model=geo_model_3,\n    x=fault2_points_3['X'].values,\n    y=fault2_points_3['Y'].values,\n    z=fault2_points_3['Z'].values,\n    elements_names=fault2_points_3['surface'].values\n)\n\ngp.add_orientations(\n    geo_model=geo_model_3,\n    x=fault2_orientations_3['X'].values,\n    y=fault2_orientations_3['Y'].values,\n    z=fault2_orientations_3['Z'].values,\n    elements_names=fault2_orientations_3['surface'].values,\n    pole_vector=np.array([[-0.966, 0, 0.259]])  # Opposite dipping fault\n)\n\n# Add stratigraphic layers\ngp.add_surface_points(\n    geo_model=geo_model_3,\n    x=surface_points_3['X'].values,\n    y=surface_points_3['Y'].values,\n    z=surface_points_3['Z'].values,\n    elements_names=surface_points_3['surface'].values\n)\n\ngp.add_orientations(\n    geo_model=geo_model_3,\n    x=orientations_3['X'].values,\n    y=orientations_3['Y'].values,\n    z=orientations_3['Z'].values,\n    elements_names=orientations_3['surface'].values,\n    pole_vector=np.array([[0, 0, 1], [0, 0, 1]])  # Horizontal layers\n)\n\n# Map geological series to surfaces\ngp.map_stack_to_surfaces(\n    gempy_model=geo_model_3,\n    mapping_object={\n        'Fault1_series': 'Fault1',\n        'Fault2_series': 'Fault2',\n        'Strata': ['Layer2', 'Layer3']\n    }\n)\n\n# Set fault relations\ngeo_model_3.structural_frame.structural_groups[0].structural_relation = gp.data.StackRelationType.FAULT\ngeo_model_3.structural_frame.structural_groups[1].structural_relation = gp.data.StackRelationType.FAULT\ngeo_model_3.structural_frame.fault_relations = np.array([[0, 1, 1], [0, 0, 1], [0, 0, 0]])\n\n# Gravity setup\nxy_grid_3 = create_gravity_grid(extent, grav_res)\ngp.set_centered_grid(\n    grid=geo_model_3.grid,\n    centers=xy_grid_3,\n    resolution=np.array([10, 10, 15]),\n    radius=np.array([2000, 2000, 2000])\n)\n\ngravity_gradient_3 = gp.calculate_gravity_gradient(geo_model_3.grid.centered_grid)\ngeo_model_3.geophysics_input = gp.data.GeophysicsInput(\n    tz=gravity_gradient_3,\n    densities=np.array([2.3, 2.6, 2.8])\n)\n\n# Compute and visualize\ngrav_3, sol_3 = compute_and_plot_gravity(geo_model_3, \"Model 3: Two Opposing Faults\", grav_res)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "z1ygyj6zcpo",
   "source": "### Model 2: Two Opposing Dipping Faults\nThe most complex model with two faults dipping in opposite directions (forming a graben or horst structure). This produces an even more distinctive gravity response.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w3is91ngmr",
   "source": "# ============================================================================\n# COMPARISON PLOT\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"GRAVITY COMPARISON\")\nprint(\"=\" * 70)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nextent_vals = geo_model_1.grid.regular_grid.extent\n\nim1 = axes[0].imshow(grav_1.reshape(grav_res, grav_res), \n                     extent=(extent_vals[0], extent_vals[1], extent_vals[2], extent_vals[3]),\n                     cmap='RdBu_r', origin='lower', aspect='auto')\naxes[0].set_title('Model 1: No Fault')\naxes[0].set_xlabel('X')\naxes[0].set_ylabel('Y')\nplt.colorbar(im1, ax=axes[0], label='μgal')\n\nim2 = axes[1].imshow(grav_2.reshape(grav_res, grav_res),\n                     extent=(extent_vals[0], extent_vals[1], extent_vals[2], extent_vals[3]),\n                     cmap='RdBu_r', origin='lower', aspect='auto')\naxes[1].set_title('Model 2: One Fault')\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('Y')\nplt.colorbar(im2, ax=axes[1], label='μgal')\n\nim3 = axes[2].imshow(grav_3.reshape(grav_res, grav_res),\n                     extent=(extent_vals[0], extent_vals[1], extent_vals[2], extent_vals[3]),\n                     cmap='RdBu_r', origin='lower', aspect='auto')\naxes[2].set_title('Model 3: Two Opposing Faults')\naxes[2].set_xlabel('X')\naxes[2].set_ylabel('Y')\nplt.colorbar(im3, ax=axes[2], label='μgal')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nGravity Statistics:\")\nprint(f\"Model 1 (No Fault):     mean={grav_1.mean():.2f}, std={grav_1.std():.2f}, range=[{grav_1.min():.2f}, {grav_1.max():.2f}]\")\nprint(f\"Model 2 (One Fault):    mean={grav_2.mean():.2f}, std={grav_2.std():.2f}, range=[{grav_2.min():.2f}, {grav_2.max():.2f}]\")\nprint(f\"Model 3 (Two Faults):   mean={grav_3.mean():.2f}, std={grav_3.std():.2f}, range=[{grav_3.min():.2f}, {grav_3.max():.2f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sw4r3wh36e8",
   "source": "### Comparison of Gravity Responses\n\nLet's compare the gravity responses from all three models. Notice how each model produces a unique gravity signature:\n- **No fault**: Smooth, symmetric gravity field\n- **One fault**: Clear linear discontinuity in the gravity field\n- **Two faults**: Two discontinuities creating a more complex pattern\n\nThese differences will allow the Trans-Conceptual sampler to distinguish between the models.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "9lt37fdf00p",
   "source": "# Trans-Conceptual Model Selection\n\nNow we'll set up a Trans-Conceptual MCMC sampling example to determine which model best explains synthetic gravity observations.\n\n**Goal**: Given noisy gravity observations, use pyTransC to identify the correct geological model (state) among the three competing hypotheses:\n- **State 0**: No fault\n- **State 1**: One fault  \n- **State 2**: Two opposing faults",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "no0l684xpy",
   "source": "# ============================================================================\n# GENERATE SYNTHETIC OBSERVATIONS\n# ============================================================================\n# Use Model 2 (One Fault) as the \"true\" model and add noise\n\nprint(\"=\" * 70)\nprint(\"GENERATING SYNTHETIC OBSERVATIONS\")\nprint(\"=\" * 70)\n\n# Choose Model 2 (one fault) as the true model\ntrue_model = 2  # State 1 in 0-indexed\ntrue_gravity = grav_2.copy()\n\n# Add Gaussian noise to create synthetic observations\nnoise_std = 0.5  # Standard deviation of measurement noise (μgal)\nnp.random.seed(123)  # For reproducibility\nnoise = np.random.normal(0, noise_std, size=true_gravity.shape)\nobserved_gravity = true_gravity + noise\n\n# Store for later use\nn_data = len(observed_gravity)\n\n# Visualize the observed data\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Plot true gravity\nim1 = axes[0].imshow(true_gravity.reshape(grav_res, grav_res),\n                     extent=(extent_vals[0], extent_vals[1], extent_vals[2], extent_vals[3]),\n                     cmap='RdBu_r', origin='lower', aspect='auto')\naxes[0].set_title('True Gravity (Model 2: One Fault)')\naxes[0].set_xlabel('X')\naxes[0].set_ylabel('Y')\nplt.colorbar(im1, ax=axes[0], label='μgal')\n\n# Plot noise\nim2 = axes[1].imshow(noise.reshape(grav_res, grav_res),\n                     extent=(extent_vals[0], extent_vals[1], extent_vals[2], extent_vals[3]),\n                     cmap='seismic', origin='lower', aspect='auto', vmin=-2*noise_std, vmax=2*noise_std)\naxes[1].set_title(f'Measurement Noise (σ={noise_std} μgal)')\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('Y')\nplt.colorbar(im2, ax=axes[1], label='μgal')\n\n# Plot observed data\nim3 = axes[2].imshow(observed_gravity.reshape(grav_res, grav_res),\n                     extent=(extent_vals[0], extent_vals[1], extent_vals[2], extent_vals[3]),\n                     cmap='RdBu_r', origin='lower', aspect='auto')\naxes[2].set_title('Observed Gravity (True + Noise)')\naxes[2].set_xlabel('X')\naxes[2].set_ylabel('Y')\nplt.colorbar(im3, ax=axes[2], label='μgal')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTrue model: State {true_model-1} (Model {true_model}: One Fault)\")\nprint(f\"Number of observations: {n_data}\")\nprint(f\"Noise level: σ = {noise_std} μgal\")\nprint(f\"SNR: {true_gravity.std() / noise_std:.2f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5yri656yop",
   "source": "---\n\n## Part 2: Creating Synthetic Observations\n\nTo test the Trans-Conceptual sampling approach, we'll create synthetic observations by:\n1. Selecting one model as the \"true\" model (we'll use Model 1: One fault)\n2. Adding realistic measurement noise to its gravity response\n3. Treating this as our \"observed\" data\n\nThe goal is then to see if the Trans-Conceptual sampler can correctly identify which model generated the data, even in the presence of noise.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "puaicw2qg6",
   "source": "# ============================================================================\n# DEFINE LOG-LIKELIHOOD AND LOG-PRIOR FUNCTIONS\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"SETTING UP TRANS-CONCEPTUAL SAMPLING\")\nprint(\"=\" * 70)\n\n# Import probability functions from module (for macOS/Windows compatibility)\nfrom functools import partial\nfrom gravity_functions import (\n    compute_gravity_for_state as _compute_gravity_for_state,\n    log_likelihood as _log_likelihood,\n    log_prior,\n    log_posterior as _log_posterior\n)\n\n# Create partial functions binding the gravity data\ncompute_gravity_for_state = partial(_compute_gravity_for_state, grav_1=grav_1, grav_2=grav_2, grav_3=grav_3)\nlog_likelihood = partial(_log_likelihood, observed_gravity=observed_gravity, noise_std=noise_std, grav_1=grav_1, grav_2=grav_2, grav_3=grav_3)\nlog_posterior = partial(_log_posterior, observed_gravity=observed_gravity, noise_std=noise_std, grav_1=grav_1, grav_2=grav_2, grav_3=grav_3)\n\nprint(\"✓ Log-likelihood and log-prior functions defined\")\nprint(f\"  - Each state has 1 parameter: amplitude (scaling factor)\")\nprint(f\"  - Prior: amplitude ~ Uniform(0.5, 1.5)\")\nprint(f\"  - Likelihood: Gaussian with fixed σ = {noise_std} μgal\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "sg4vzaud88f",
   "source": "---\n\n## Part 3: Bayesian Inference Setup\n\n### Parameterization\n\nFor this example, we keep the parameterization simple:\n- **Each model has 1 parameter**: An amplitude scaling factor that represents uncertainty in the density contrast between layers\n- **Physical interpretation**: Real density contrasts are uncertain, so this parameter allows each model to optimize its fit to the data\n\n### Likelihood Function\n\nWe use a Gaussian likelihood:\n```\np(data | model, params) ∝ exp(-0.5 * ||observed - predicted||² / σ²)\n```\n\nwhere σ is the known measurement noise standard deviation.\n\n### Prior Distribution\n\nWe use a uniform prior on the amplitude: `amplitude ~ Uniform(0.5, 1.5)`, allowing for ±50% uncertainty in density contrast.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ld7640rizts",
   "source": "# ============================================================================\n# SETUP TRANS-CONCEPTUAL SAMPLING PARAMETERS\n# ============================================================================\n\n# Import pyTransC samplers\nfrom pytransc.samplers import run_mcmc_per_state, run_ensemble_resampler\nfrom pytransc.utils.auto_pseudo import build_auto_pseudo_prior\nfrom pytransc.analysis.visits import get_visits_to_states\n\n# Sampling parameters\nnstates = 3  # Number of competing models\nndims = [1, 1, 1]  # Each state has 1 parameter (amplitude)\nnwalkers = 32  # Number of MCMC walkers per state\nnsteps_per_state = 1000  # Steps for within-state sampling\nnsteps_ensemble = 10000  # Steps for ensemble resampler\n\n# Initialize walker starting positions for each state\n# Start walkers around amplitude=1.0 with small perturbations\nnp.random.seed(42)\npos = []\nfor state in range(nstates):\n    # Create starting positions: amplitude ~ Uniform(0.9, 1.1)\n    pos_state = np.random.uniform(0.9, 1.1, size=(nwalkers, 1))\n    pos.append(pos_state)\n    \n    # Test the log-posterior at the first walker position\n    test_params = pos_state[0]\n    lp = log_prior(test_params, state)\n    ll = log_likelihood(test_params, state)\n    lpost = lp + ll\n    print(f\"State {state}: log-prior={lp:.2f}, log-likelihood={ll:.2f}, log-posterior={lpost:.2f}\")\n\nprint(f\"\\n✓ Sampling configuration complete\")\nprint(f\"  - Number of states: {nstates}\")\nprint(f\"  - Dimensions per state: {ndims}\")\nprint(f\"  - Walkers per state: {nwalkers}\")\nprint(f\"  - Steps for within-state sampling: {nsteps_per_state}\")\nprint(f\"  - Steps for ensemble resampler: {nsteps_ensemble}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mho7z3kh70q",
   "source": "---\n\n## Part 4: Trans-Conceptual MCMC Sampling\n\n### Two-Stage Approach\n\nWe use the **ensemble resampler** methodology:\n\n**Stage 1: Within-State Sampling**\n- Run standard MCMC independently for each model\n- This builds a posterior ensemble for each model's parameter space\n- Goal: Learn about each model's parameter distribution given the data\n\n**Stage 2: Trans-Conceptual Sampling**\n- Use the posterior ensembles to construct \"pseudo-priors\" (bridge distributions)\n- Run ensemble resampler to jump between models\n- Goal: Estimate posterior probability of each model\n\n### Why This Approach?\n\nThe two-stage approach avoids expensive forward model evaluations during trans-conceptual jumps by:\n1. Pre-computing parameter samples for each model\n2. Resampling from these ensembles during model jumps\n3. Using pseudo-priors to ensure detailed balance and correct posterior probabilities",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o4cga5lc51",
   "source": "# ============================================================================\n# STAGE 1: RUN MCMC PER STATE TO BUILD POSTERIOR ENSEMBLES\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"STAGE 1: WITHIN-STATE MCMC SAMPLING\")\nprint(\"=\" * 70)\n\nimport time\n\nstart_time = time.time()\n\n# Run MCMC independently for each state\nensembles, log_probs = run_mcmc_per_state(\n    n_states=nstates,\n    n_dims=ndims,\n    n_walkers=nwalkers,\n    n_steps=nsteps_per_state,\n    pos=pos,\n    log_posterior=log_posterior,\n    verbose=True,\n    skip_initial_state_check=True,\n)\n\nelapsed_time = time.time() - start_time\n\nprint(f\"\\n✓ Within-state sampling completed in {elapsed_time:.2f} seconds\")\nprint(f\"\\nEnsemble shapes:\")\nfor i, ens in enumerate(ensembles):\n    print(f\"  State {i}: {ens.shape} (walkers × steps, dims)\")\nprint(f\"\\nLog-probability shapes:\")\nfor i, lp in enumerate(log_probs):\n    print(f\"  State {i}: {lp.shape}\")\n    \n# Visualize posterior distributions for each state\nfig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\nfor state in range(nstates):\n    ax = axes[state]\n    \n    # Extract amplitude samples\n    amplitude_samples = ensembles[state][:, 0]\n    \n    # Plot histogram\n    ax.hist(amplitude_samples, bins=50, density=True, alpha=0.7, edgecolor='black')\n    ax.axvline(1.0, color='r', linestyle='--', linewidth=2, label='True value')\n    ax.set_xlabel('Amplitude')\n    ax.set_ylabel('Density')\n    ax.set_title(f'State {state} Posterior\\n({\"No fault\" if state==0 else \"One fault\" if state==1 else \"Two faults\"})')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \nplt.tight_layout()\nplt.show()\n\n# Print summary statistics\nprint(\"\\nPosterior summary statistics:\")\nfor state in range(nstates):\n    amp_samples = ensembles[state][:, 0]\n    print(f\"  State {state}: mean={amp_samples.mean():.4f}, std={amp_samples.std():.4f}, \"\n          f\"95% CI=[{np.percentile(amp_samples, 2.5):.4f}, {np.percentile(amp_samples, 97.5):.4f}]\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "s0lvi5eb4ko",
   "source": "### Stage 1: Within-State Sampling\n\nWe now run MCMC independently for each of the three models using `run_mcmc_per_state()`. This function uses emcee (the affine-invariant ensemble sampler) to explore the parameter space of each model.\n\n**What to expect:**\n- Each model will find an optimal amplitude parameter that best fits the data\n- Model 1 (one fault) should achieve the best fit since it generated the data\n- Models 0 and 2 will try to compensate with different amplitude values but won't fit as well",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hhvsrz2bw7f",
   "source": "# ============================================================================\n# STAGE 2: BUILD PSEUDO-PRIORS FROM POSTERIOR ENSEMBLES\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"STAGE 2: BUILDING PSEUDO-PRIORS\")\nprint(\"=\" * 70)\n\nfrom scipy import stats\n\n# Build pseudo-priors using Gaussian approximation\n# For each state, fit a Gaussian to the posterior ensemble\n\nlog_pseudo_prior_ens = []\n\nfor state in range(nstates):\n    # Get ensemble for this state\n    ens = ensembles[state]\n    \n    # Fit Gaussian: mean and covariance\n    mean = np.mean(ens, axis=0)\n    cov = np.diag(np.var(ens, axis=0))  # Diagonal covariance\n    \n    # Create multivariate normal distribution\n    rv = stats.multivariate_normal(mean=mean, cov=cov)\n    \n    # Evaluate pseudo-prior log-probability for all ensemble members\n    log_pseudo_prior_ens.append(rv.logpdf(ens))\n    \n    print(f\"State {state}: mean={mean[0]:.4f}, std={np.sqrt(cov[0,0]):.4f}\")\n\nprint(f\"\\n✓ Pseudo-priors built for all {nstates} states\")\n\n# Prepare data for ensemble resampler\nensemble_per_state = ensembles\nlog_posterior_ens = log_probs\n\nprint(f\"\\nReady for ensemble resampler:\")\nfor i in range(nstates):\n    print(f\"  State {i}: {len(ensemble_per_state[i])} samples\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6o3epkhndgw",
   "source": "### Stage 2: Building Pseudo-Priors\n\nThe pseudo-priors are bridge distributions that enable valid transitions between models. We construct them by fitting Gaussian distributions to each posterior ensemble.\n\n**Key concept:** The pseudo-prior for state *i* approximates the posterior distribution p(θ_i | data, state i). This allows us to:\n1. Propose parameters when jumping TO state *i*\n2. Calculate acceptance ratios that maintain detailed balance\n3. Correctly estimate posterior model probabilities\n\nFor this simple 1D problem, we use diagonal Gaussian approximations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "iw2a7a8ixg",
   "source": "# ============================================================================\n# STAGE 3: RUN ENSEMBLE RESAMPLER FOR TRANS-CONCEPTUAL MODEL SELECTION\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"STAGE 3: TRANS-CONCEPTUAL ENSEMBLE RESAMPLER\")\nprint(\"=\" * 70)\n\nstart_time = time.time()\n\n# Run ensemble resampler\ner_results = run_ensemble_resampler(\n    n_walkers=16,  # Number of walkers for ensemble resampler\n    n_steps=nsteps_ensemble,\n    n_states=nstates,\n    n_dims=ndims,\n    log_posterior_ens=log_posterior_ens,\n    log_pseudo_prior_ens=log_pseudo_prior_ens,\n    parallel=False,\n    progress=True\n)\n\nelapsed_time = time.time() - start_time\n\nprint(f\"\\n✓ Ensemble resampler completed in {elapsed_time:.2f} seconds\")\n\n# Extract results\nstate_chains = er_results.state_chain  # (n_walkers, n_steps)\nn_accepted = er_results.n_accepted\nn_proposed = er_results.n_proposed\nacceptance_rates = n_accepted / n_proposed * 100\n\nprint(f\"\\nState chain shape: {state_chains.shape}\")\nprint(f\"Average acceptance rate: {np.mean(acceptance_rates):.2f}%\")\nprint(f\"Acceptance rate range: {np.min(acceptance_rates):.2f}% - {np.max(acceptance_rates):.2f}%\")\n\n# Calculate state visitation frequencies\nstate_visits = np.bincount(state_chains.flatten(), minlength=nstates)\nstate_frequencies = state_visits / state_visits.sum()\n\nprint(f\"\\nState visitation frequencies:\")\nstate_names = [\"No fault\", \"One fault\", \"Two faults\"]\nfor i in range(nstates):\n    print(f\"  State {i} ({state_names[i]:12s}): {state_frequencies[i]:.4f} ({state_visits[i]:,} visits)\")\n\nprint(f\"\\nTrue model was State 1 (One fault)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d8kzwdjci4w",
   "source": "### Stage 3: Trans-Conceptual Sampling with Ensemble Resampler\n\nNow we run the ensemble resampler using `run_ensemble_resampler()`. This sampler:\n\n1. **Proposes model changes**: Randomly selects a new state (model) to jump to\n2. **Resamples parameters**: Draws a parameter value from the target state's ensemble\n3. **Calculates acceptance probability**: Using the Metropolis-Hastings ratio with pseudo-priors\n4. **Accepts/rejects**: Maintains detailed balance to correctly sample the trans-conceptual posterior\n\n**What to expect:**\n- The chain will visit all three states\n- Model 1 (one fault) should be visited most frequently since it's the true model\n- The visitation frequencies approximate the posterior model probabilities",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gjclrtloa3",
   "source": "# ============================================================================\n# VISUALIZATION AND ANALYSIS OF TRANS-CONCEPTUAL RESULTS\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"RESULTS ANALYSIS\")\nprint(\"=\" * 70)\n\n# Create comprehensive visualization\nfig = plt.figure(figsize=(18, 10))\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. State trace plot (top row, spanning all columns)\nax1 = fig.add_subplot(gs[0, :])\nfor walker_idx in range(state_chains.shape[0]):\n    ax1.plot(state_chains[walker_idx, :], alpha=0.3, linewidth=0.5)\nax1.axhline(y=1, color='r', linestyle='--', linewidth=2, label='True state (One fault)')\nax1.set_xlabel('Iteration')\nax1.set_ylabel('State Index')\nax1.set_title('State Trace: Trans-Conceptual MCMC Chains')\nax1.set_yticks([0, 1, 2])\nax1.set_yticklabels(['No fault', 'One fault', 'Two faults'])\nax1.grid(True, alpha=0.3)\nax1.legend()\n\n# 2. State histogram (middle left)\nax2 = fig.add_subplot(gs[1, 0])\n# Use all samples after burn-in (skip first 10%)\nburnin = int(0.1 * state_chains.shape[1])\nstate_samples_burnin = state_chains[:, burnin:].flatten()\ncounts, bins, patches = ax2.hist(state_samples_burnin, bins=np.arange(nstates+1)-0.5, \n                                   density=True, alpha=0.7, edgecolor='black')\n# Color the true state bar in red\npatches[1].set_facecolor('red')\npatches[1].set_alpha(0.8)\nax2.set_xlabel('State Index')\nax2.set_ylabel('Probability Density')\nax2.set_title(f'State Posterior Distribution\\n(after {burnin} step burn-in)')\nax2.set_xticks([0, 1, 2])\nax2.set_xticklabels(['No\\nfault', 'One\\nfault', 'Two\\nfaults'])\nax2.grid(True, alpha=0.3, axis='y')\n\n# Add probability text on bars\nstate_probs_burnin = np.bincount(state_samples_burnin.astype(int), minlength=nstates) / len(state_samples_burnin)\nfor i, prob in enumerate(state_probs_burnin):\n    ax2.text(i, prob + 0.05, f'{prob:.3f}', ha='center', fontsize=12, fontweight='bold')\n\n# 3. Cumulative state probability (middle center)\nax3 = fig.add_subplot(gs[1, 1])\ncumulative_visits = np.zeros((nstates, state_chains.shape[1]))\nfor step in range(state_chains.shape[1]):\n    visits = np.bincount(state_chains[:, :step+1].flatten(), minlength=nstates)\n    cumulative_visits[:, step] = visits / visits.sum()\n\ncolors = ['blue', 'red', 'green']\nfor state in range(nstates):\n    ax3.plot(cumulative_visits[state, :], label=state_names[state], \n             color=colors[state], linewidth=2, alpha=0.8)\nax3.set_xlabel('Iteration')\nax3.set_ylabel('Cumulative Probability')\nax3.set_title('Convergence of State Probabilities')\nax3.legend()\nax3.grid(True, alpha=0.3)\nax3.set_ylim([0, 1])\n\n# 4. Model comparison via Bayes factors (middle right)\nax4 = fig.add_subplot(gs[1, 2])\n# Calculate Bayes factors relative to State 1 (true model)\n# BF_ij = P(State i) / P(State j)\nbayes_factors = state_probs_burnin / state_probs_burnin[1]  # Relative to State 1\nbar_colors = ['blue' if i != 1 else 'red' for i in range(nstates)]\nbars = ax4.bar(range(nstates), bayes_factors, color=bar_colors, alpha=0.7, edgecolor='black')\nax4.axhline(y=1, color='gray', linestyle='--', linewidth=1)\nax4.set_xlabel('State Index')\nax4.set_ylabel('Bayes Factor (relative to State 1)')\nax4.set_title('Bayes Factors\\n(Evidence Ratios)')\nax4.set_xticks([0, 1, 2])\nax4.set_xticklabels(['No\\nfault', 'One\\nfault', 'Two\\nfaults'])\nax4.set_yscale('log')\nax4.grid(True, alpha=0.3, axis='y')\n\n# Add values on bars\nfor i, bf in enumerate(bayes_factors):\n    ax4.text(i, bf * 1.2, f'{bf:.2f}', ha='center', fontsize=10, fontweight='bold')\n\n# 5-7. Gravity residuals for each model (bottom row)\nfor state in range(nstates):\n    ax = fig.add_subplot(gs[2, state])\n    \n    # Compute predicted gravity for this state using mean amplitude from posterior\n    mean_amplitude = ensembles[state][:, 0].mean()\n    predicted = compute_gravity_for_state(state, mean_amplitude)\n    residual = observed_gravity - predicted\n    \n    # Plot residual map\n    im = ax.imshow(residual.reshape(grav_res, grav_res),\n                   extent=(extent_vals[0], extent_vals[1], extent_vals[2], extent_vals[3]),\n                   cmap='seismic', origin='lower', aspect='auto',\n                   vmin=-2*noise_std, vmax=2*noise_std)\n    ax.set_title(f'{state_names[state]}\\nRMS={np.sqrt(np.mean(residual**2)):.3f} μgal')\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    plt.colorbar(im, ax=ax, label='Residual (μgal)')\n\nplt.suptitle('Trans-Conceptual MCMC: Gravity Model Selection', fontsize=16, fontweight='bold', y=0.995)\nplt.show()\n\n# Print final summary\nprint(\"\\n\" + \"=\" * 70)\nprint(\"FINAL SUMMARY\")\nprint(\"=\" * 70)\nprint(f\"\\nTrue model: State 1 (One fault)\")\nprint(f\"\\nPosterior probabilities (after burn-in):\")\nfor i in range(nstates):\n    print(f\"  State {i} ({state_names[i]:12s}): {state_probs_burnin[i]:.4f}\")\nprint(f\"\\nBayes factors (relative to State 1):\")\nfor i in range(nstates):\n    print(f\"  State {i} ({state_names[i]:12s}): {bayes_factors[i]:.2f}\")\n    \n# Interpretation\nif state_probs_burnin[1] > 0.5:\n    print(f\"\\n✓ SUCCESS: Trans-Conceptual sampling correctly identified State 1 (One fault)\")\n    print(f\"  with posterior probability {state_probs_burnin[1]:.4f}\")\nelse:\n    print(f\"\\n⚠ CAUTION: Trans-Conceptual sampling assigned highest probability to State {np.argmax(state_probs_burnin)}\")\n    print(f\"  but true model was State 1\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6q3ef4ooez6",
   "source": "---\n\n## Conclusions and Next Steps\n\n### What We Demonstrated\n\nThis notebook showed how Trans-Conceptual MCMC can:\n1. **Simultaneously infer parameters and select models** - No need to run separate analyses for each model\n2. **Quantify model uncertainty** - Get posterior probabilities, not just point estimates\n3. **Handle model complexity** - Automatically account for different model complexities through the evidence\n\n### Extensions and Variations\n\nYou can modify this example to explore:\n\n**1. More complex parameterizations:**\n- Add more parameters (fault location, dip angle, layer depths, density values)\n- Use more realistic priors based on geological knowledge\n\n**2. Different noise levels:**\n- Increase noise to see when models become indistinguishable\n- Add correlated noise to simulate systematic errors\n\n**3. More models:**\n- Add models with different numbers of layers\n- Include models with intrusions or other geological features\n\n**4. Alternative samplers:**\n- Try `run_product_space_sampler()` for simultaneous parameter space exploration\n- Try `run_state_jump_sampler()` for RJ-MCMC style trans-dimensional sampling\n\n**5. Real data:**\n- Replace synthetic observations with real gravity survey data\n- Incorporate data uncertainties and measurement locations\n\n### Key Takeaways\n\n✓ Trans-Conceptual sampling provides a principled way to compare models with different structures\n\n✓ The ensemble resampler is efficient for problems where forward model evaluation is expensive\n\n✓ Posterior model probabilities account for both fit quality and model complexity (via Occam's razor)\n\n✓ The methodology naturally handles model uncertainty in geophysical inverse problems",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "qi64aoij7",
   "source": "---\n\n## Part 5: Results and Model Selection\n\n### Interpreting the Results\n\nThe ensemble resampler output provides:\n\n1. **State chains**: Trace of which model each walker visited at each iteration\n2. **Posterior model probabilities**: Fraction of samples in each state (after burn-in)\n3. **Bayes factors**: Relative evidence ratios between models\n4. **Acceptance rates**: Diagnostic for sampler efficiency\n\n**Success criteria:**\n- Model 1 (one fault) should have the highest posterior probability\n- Bayes factors should strongly favor Model 1\n- State chains should show good mixing between models\n- Gravity residuals should be smallest for Model 1",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}